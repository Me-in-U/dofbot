# Dofbot Pick-and-Place V2 Training Guide

## ğŸ¯ V1 vs V2 ì£¼ìš” ì°¨ì´ì 

### V1 ë¬¸ì œì  ë¶„ì„

- âœ… **Reach ë‹¨ê³„**: í•™ìŠµë¨
- âŒ **Pick ë‹¨ê³„**: í•™ìŠµ ì•ˆë¨
- âŒ **Move ë‹¨ê³„**: í•™ìŠµ ì•ˆë¨

### V2 ê°œì„  ì‚¬í•­

#### 1. **Curriculum Learning (ë‹¨ê³„ì  í•™ìŠµ)**

```python
Stage 1: Reach    â†’ EEë¥¼ ë¬¼ì²´ì— ì ‘ê·¼
Stage 2: Grasp    â†’ ê·¸ë¦¬í¼ë¡œ ë¬¼ì²´ ì¡ê¸°
Stage 3: Lift     â†’ ë¬¼ì²´ë¥¼ í…Œì´ë¸”ì—ì„œ ë“¤ì–´ì˜¬ë¦¬ê¸°
Stage 4: Transportâ†’ ì¡ì€ ë¬¼ì²´ë¥¼ ëª©í‘œë¡œ ì´ë™
Stage 5: Place    â†’ ëª©í‘œ ìœ„ì¹˜ì— ë†“ê¸°
```

#### 2. **ë” ëª…í™•í•œ Reward êµ¬ì¡°**

```yaml
V1 (ë³µì¡í•œ reward):
  - 8ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ reward í•­ëª©
  - ìƒí˜¸ì‘ìš©ì´ ë¶ˆëª…í™•

V2 (ë‹¨ê³„ë³„ reward):
  - ê° stageë§ˆë‹¤ ëª…í™•í•œ reward
  - ì§„í–‰ ìƒí™©ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë‹¤ìŒ stageë¡œ ì „í™˜
  - Sparse bonus + Dense shaping ì¡°í•©
```

#### 3. **Policy Network ê°œì„ **

```yaml
V1: [256, 256, 128]
V2: [512, 512, 256, 128] # ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬
```

#### 4. **ë” ë‚˜ì€ Exploration**

```yaml
V1:
  - initial_log_std: -0.5
  - entropy_loss_scale: 0.001
  - rollouts: 64

V2:
  - initial_log_std: 0.0 # ì´ˆê¸° íƒí—˜ ì¦ê°€
  - entropy_loss_scale: 0.005 # 5ë°° ë†’ì€ entropy
  - rollouts: 128 # 2ë°° ë§ì€ rollouts
```

#### 5. **ë¬¼ë¦¬ íŒŒë¼ë¯¸í„° ê°œì„ **

```python
V1:
  - object mass: 0.05kg
  - object size: 0.03m
  - damping: 40.0, 10.0

V2:
  - object mass: 0.03kg      # ë” ê°€ë²¼ì›€
  - object size: 0.025m      # ë” ì‘ìŒ
  - damping: 50.0, 15.0      # ë” ì•ˆì •ì 
```

---

## ğŸš€ V2 í•™ìŠµ ì‹¤í–‰

### ê¸°ë³¸ í•™ìŠµ (ê¶Œì¥)

```bash
python scripts/skrl/train.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --algorithm=PPO \
  --ml_framework=torch \
  --num_envs=1024 \
  --device=cuda
```

### ë” ë§ì€ í™˜ê²½ìœ¼ë¡œ í•™ìŠµ (GPU ì—¬ìœ  ìˆì„ ë•Œ)

```bash
python scripts/skrl/train.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --algorithm=PPO \
  --ml_framework=torch \
  --num_envs=2048 \
  --device=cuda
```

### Headless ëª¨ë“œ (ë” ë¹ ë¥¸ í•™ìŠµ)

```bash
python scripts/skrl/train.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --algorithm=PPO \
  --ml_framework=torch \
  --num_envs=1024 \
  --device=cuda \
  --headless
```

---

## ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§

### TensorBoard ì‹¤í–‰

```bash
# ìƒˆ í„°ë¯¸ë„ ì—´ê¸°
cd C:\Users\Zoe_Lowell\Documents\GitHub\DofBot-Issac-Sim\rl\dofbot_isaacLab\dofbot

tensorboard --logdir=logs/skrl/dofbot_pickplace_direct_v2
```

ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:6006` ì ‘ì†

### ì£¼ìš” ëª¨ë‹ˆí„°ë§ ì§€í‘œ

#### 1. **Stage ì§„í–‰ ìƒí™©** (ì¶”ê°€ êµ¬í˜„ í•„ìš”)

í•™ìŠµì´ ê° stageë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì§„í–‰í•˜ëŠ”ì§€ í™•ì¸

#### 2. **Reward í•­ëª©ë³„ ë¶„ì„**

```
- Reward/Total: ì „ì²´ ë³´ìƒ (ì ì§„ì  ì¦ê°€ ê¸°ëŒ€)
- Stage1/Reach: Reach ì„±ê³µë¥ 
- Stage2/Grasp: Grasp ì„±ê³µë¥ 
- Stage3/Lift: Lift ì„±ê³µë¥ 
- Stage4/Transport: Transport ì§„í–‰ë„
- Stage5/Place: Place ì„±ê³µë¥ 
```

#### 3. **Policy Loss**

- ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œí•´ì•¼ í•¨
- ê¸‰ê²©í•œ ë³€í™”ëŠ” learning rate ë¬¸ì œ

#### 4. **Episode Length**

- ì´ˆê¸°: ~900 (15ì´ˆ \* 60 FPS)
- í•™ìŠµ í›„: ì ì  ì§§ì•„ì§ (ë¹ ë¥¸ ì„±ê³µ)

---

## ğŸ“ Curriculum Learning ì‘ë™ ë°©ì‹

### Stage ìë™ ì „í™˜

```python
# í™˜ê²½ ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ stage ì¶”ì 
self._current_stage[env_i]:
  - 0: ì•„ì§ ë„ë‹¬ ì•ˆí•¨
  - 1: ë„ë‹¬ ì™„ë£Œ (reached)
  - 2: ì¡ê¸° ì™„ë£Œ (grasped)
  - 3: ë“¤ì–´ì˜¬ë¦¼ ì™„ë£Œ (lifted)
  - 4: ëª©í‘œ ê·¼ì²˜ (near_goal)
```

### Reward ë³€í™”

```python
# Stage 1: ì£¼ë¡œ reach reward
reward = -2.0 * d_ee_obj + 3.0 * reached

# Stage 2: grasp reward ì¶”ê°€
reward += 2.0 * gripper_closure + 5.0 * grasped

# Stage 3: lift reward ì¶”ê°€
reward += 4.0 * lift_progress + 3.0 * lifted

# Stage 4: transport reward ì¶”ê°€
reward += -1.5 * d_obj_goal + 2.0 * goal_proximity

# Stage 5: place bonus
reward += 10.0 * placed
```

---

## ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¬ë©´

```yaml
# agents/skrl_ppo_pickplace_v2_cfg.yaml

# 1. Learning rate ì¦ê°€
learning_rate: 3.0e-04  # 1e-4 â†’ 3e-4

# 2. Rollouts ì¦ê°€
rollouts: 256  # 128 â†’ 256

# 3. í™˜ê²½ ìˆ˜ ì¦ê°€
--num_envs=2048  # 1024 â†’ 2048
```

### Graspì´ ì•ˆë˜ë©´

```python
# dofbot_pickplace_env_cfg_v2.py

# 1. Grasp reward ì¦ê°€
rew_stage2_grasp_bonus = 10.0  # 5.0 â†’ 10.0

# 2. Grasp threshold ì™„í™”
grasp_threshold = 0.05  # 0.04 â†’ 0.05

# 3. Gripper actuator ê°•í™”
damping=20.0  # 15.0 â†’ 20.0
```

### Liftê°€ ì•ˆë˜ë©´

```python
# 1. Lift reward ì¦ê°€
rew_stage3_lift = 6.0  # 4.0 â†’ 6.0

# 2. Objectë¥¼ ë” ê°€ë³ê²Œ
mass=0.02  # 0.03 â†’ 0.02

# 3. Lift threshold ë‚®ì¶¤
lift_threshold = 0.06  # 0.08 â†’ 0.06
```

### Policyê°€ ë¶ˆì•ˆì •í•˜ë©´

```yaml
# 1. Learning rate ê°ì†Œ
learning_rate: 5.0e-05 # 1e-4 â†’ 5e-5

# 2. Gradient clipping ê°•í™”
grad_norm_clip: 0.3 # 0.5 â†’ 0.3

# 3. Entropy ê°ì†Œ
entropy_loss_scale: 0.002 # 0.005 â†’ 0.002
```

---

## ğŸ“ˆ ì˜ˆìƒ í•™ìŠµ ì‹œê°„

### RTX 5070 ê¸°ì¤€ (1024 envs)

```
Total timesteps: 500,000
FPS: ~15-20 (V2ëŠ” ë” ë³µì¡í•œ ì—°ì‚°)

ì˜ˆìƒ ì‹œê°„: ~7-9ì‹œê°„
Checkpoint ê°„ê²©: 50,000 steps (~1ì‹œê°„)
```

### í•™ìŠµ ë‹¨ê³„ë³„ ì˜ˆìƒ ê²°ê³¼

```
50K steps  (~1h):  Reach í•™ìŠµ ì™„ë£Œ
150K steps (~3h):  Grasp í•™ìŠµ ì‹œì‘
250K steps (~5h):  Lift í•™ìŠµ ì™„ë£Œ
350K steps (~7h):  Transport í•™ìŠµ ì‹œì‘
500K steps (~9h):  Place ì„±ê³µë¥  30-50%
```

---

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### ìµœì†Œ ì„±ê³µ ê¸°ì¤€

```
- Reach success: >90%
- Grasp success: >60%
- Lift success: >40%
- Place success: >20%
```

### ëª©í‘œ ì„±ê³µ ê¸°ì¤€

```
- Reach success: >95%
- Grasp success: >80%
- Lift success: >60%
- Place success: >40%
```

---

## ğŸ› Troubleshooting

### ë¬¸ì œ: "ReachëŠ” ë˜ëŠ”ë° Graspì´ ì•ˆë¨"

**í•´ê²°ì±…:**

1. `rew_stage2_grasp_bonus` ì¦ê°€ (5.0 â†’ 10.0)
2. `grasp_threshold` ì¦ê°€ (0.04 â†’ 0.06)
3. ê·¸ë¦¬í¼ ì´ˆê¸° ìœ„ì¹˜ë¥¼ ì•½ê°„ ë‹«íŒ ìƒíƒœë¡œ (`grip_joint: -0.5`)

### ë¬¸ì œ: "Graspì€ ë˜ëŠ”ë° Liftê°€ ì•ˆë¨"

**í•´ê²°ì±…:**

1. Object mass ê°ì†Œ (0.03 â†’ 0.02)
2. `rew_stage3_lift` ì¦ê°€ (4.0 â†’ 6.0)
3. Arm actuator effort limit ì¦ê°€ (50 â†’ 60)

### ë¬¸ì œ: "LiftëŠ” ë˜ëŠ”ë° Transportê°€ ì•ˆë¨"

**í•´ê²°ì±…:**

1. `rew_stage4_progress` ì¦ê°€ (2.0 â†’ 4.0)
2. Episode length ì¦ê°€ (15ì´ˆ â†’ 20ì´ˆ)
3. Goal-object separation ê°ì†Œ (0.25 â†’ 0.20)

### ë¬¸ì œ: "í•™ìŠµì´ ì „í˜€ ì•ˆë¨"

**í•´ê²°ì±…:**

1. V1ìœ¼ë¡œ ëŒì•„ê°€ì„œ Reachë¶€í„° ë‹¤ì‹œ í™•ì¸
2. `initial_log_std` ì¦ê°€ (0.0 â†’ 0.5) - ë” ë§ì€ exploration
3. Learning rate ê°ì†Œ (1e-4 â†’ 5e-5) - ì•ˆì •ì„± ì¦ê°€

---

## ğŸ“ ì €ì¥ëœ ëª¨ë¸ ìœ„ì¹˜

```
logs/skrl/dofbot_pickplace_direct_v2/
â””â”€â”€ YYYY-MM-DD_HH-MM-SS_ppo_torch/
    â”œâ”€â”€ checkpoints/
    â”‚   â”œâ”€â”€ agent_50000.pt
    â”‚   â”œâ”€â”€ agent_100000.pt
    â”‚   â”œâ”€â”€ ...
    â”‚   â””â”€â”€ best_agent.pt
    â”œâ”€â”€ runs/
    â””â”€â”€ config.yaml
```

---

## ğŸ¬ V2 í‰ê°€ ì‹¤í–‰

```bash
# ìµœê³  ëª¨ë¸ í‰ê°€
python scripts/skrl/eval.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --num_envs=64 \
  --checkpoint=logs/skrl/dofbot_pickplace_direct_v2/YYYY-MM-DD_HH-MM-SS_ppo_torch/checkpoints/best_agent.pt

# íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ í‰ê°€
python scripts/skrl/eval.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --num_envs=64 \
  --checkpoint=logs/skrl/dofbot_pickplace_direct_v2/YYYY-MM-DD_HH-MM-SS_ppo_torch/checkpoints/agent_500000.pt
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### V2 ì„¤ê³„ ê¸°ë°˜ ë…¼ë¬¸

- **Curriculum Learning**: "Automatic Curriculum Learning For Deep RL"
- **Reward Shaping**: "Policy Invariance Under Reward Transformations"
- **Manipulation Learning**: "Learning Dexterous Manipulation from Suboptimal Experts"

### V1 ëŒ€ë¹„ V2 ë³€ê²½ì‚¬í•­ ìš”ì•½

| í•­ëª©           | V1              | V2                | ì´ìœ              |
| -------------- | --------------- | ----------------- | ---------------- |
| Reward êµ¬ì¡°    | ë³µì¡í•œ 8ê°œ í•­ëª© | 5ë‹¨ê³„ curriculum  | ëª…í™•í•œ í•™ìŠµ ëª©í‘œ |
| Network        | [256,256,128]   | [512,512,256,128] | ë³µì¡í•œ task ëŒ€ì‘ |
| Exploration    | entropy=0.001   | entropy=0.005     | Grasp íƒìƒ‰ ì¦ê°€  |
| Rollouts       | 64              | 128               | ì•ˆì •ì  í•™ìŠµ      |
| Training steps | 200K            | 500K              | Curriculum ì™„ë£Œ  |
| Object mass    | 0.05kg          | 0.03kg            | Grasp ìš©ì´       |
| Episode length | 10s             | 15s               | ì¶©ë¶„í•œ ì‹œê°„      |

---

## âœ¨ V2 ì‚¬ìš© ì‹œì‘í•˜ê¸°

```bash
# 1. V2 í™˜ê²½ í•™ìŠµ ì‹œì‘
python scripts/skrl/train.py --task=Dofbot-PickPlace-Direct-v2 --num_envs=1024 --device=cuda

# 2. ë³„ë„ í„°ë¯¸ë„ì—ì„œ TensorBoard ì‹¤í–‰
tensorboard --logdir=logs/skrl/dofbot_pickplace_direct_v2

# 3. í•™ìŠµ ëª¨ë‹ˆí„°ë§
# - http://localhost:6006 ì ‘ì†
# - Total rewardê°€ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸
# - Policy lossê°€ ì•ˆì •ì ì¸ì§€ í™•ì¸

# 4. 7-9ì‹œê°„ í›„ ê²°ê³¼ í™•ì¸
python scripts/skrl/eval.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --num_envs=64 \
  --checkpoint=logs/skrl/dofbot_pickplace_direct_v2/.../checkpoints/best_agent.pt
```

Good luck! ğŸš€
