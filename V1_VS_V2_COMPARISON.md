# V1 vs V2 í™˜ê²½ ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

## ğŸ“Œ ë¹ ë¥¸ ì„ íƒ ê°€ì´ë“œ

### V1 ì‚¬ìš© ê¶Œì¥ ìƒí™©

- âœ… Reach í•™ìŠµë§Œ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ
- âœ… ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì´ í•„ìš”í•  ë•Œ
- âœ… í•™ìŠµ ì‹œê°„ì´ ì œí•œì ì¼ ë•Œ (3-4ì‹œê°„)

### V2 ì‚¬ìš© ê¶Œì¥ ìƒí™© (í˜„ì¬ ìƒí™©)

- âœ… **Pick and Place ì™„ì „ í•™ìŠµì´ ëª©í‘œì¼ ë•Œ** â† **ì§€ê¸ˆ ì´ ìƒí™©**
- âœ… Grasp, Lift, Transportë¥¼ ëª¨ë‘ í•™ìŠµí•˜ê³  ì‹¶ì„ ë•Œ
- âœ… ì¶©ë¶„í•œ í•™ìŠµ ì‹œê°„ì´ ìˆì„ ë•Œ (7-9ì‹œê°„)
- âœ… ì‹¤ì œ ë¡œë´‡ ë°°í¬ë¥¼ ëª©í‘œë¡œ í•  ë•Œ

---

## ğŸ” V1 ë¬¸ì œì  ë¶„ì„ (3ì‹œê°„ í•™ìŠµ ê²°ê³¼)

```
ì‚¬ìš©ì í”¼ë“œë°±: "REACHê¹Œì§€ëŠ” ì–´ëŠì •ë„ ë˜ëŠ”ê²ƒ ê°™ì€ë° PICK AND MOVEê°€ ì•ˆëœë‹¤"
```

### V1ì—ì„œ í•™ìŠµ ì‹¤íŒ¨í•œ ì´ìœ 

#### 1. **Rewardê°€ ë„ˆë¬´ ë³µì¡í•¨**

```python
# V1: 8ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ rewardê°€ ë™ì‹œì— ì‘ìš©
reward = (
    alive * 0.1 +
    reach * -1.0 +
    close * 1.5 +
    lift * 3.0 +
    goal_track * -2.0 +
    penalty_open * -0.5 +
    bonus_grasp * 2.0 +
    bonus_place * 5.0
)
# ë¬¸ì œ: ë¡œë´‡ì´ ì–´ëŠ rewardë¥¼ ìš°ì„ í•´ì•¼ í• ì§€ ëª¨ë¦„
```

#### 2. **Grasp ê°ì§€ê°€ ë¶ˆí™•ì‹¤í•¨**

```python
# V1: ê±°ë¦¬ ê¸°ë°˜ grasp íŒë‹¨
grasp_candidate = (d_reach < 0.03) & (grip < -0.30)
# ë¬¸ì œ: ì‹¤ì œë¡œ ë¬¼ì²´ë¥¼ ì¡ì•˜ëŠ”ì§€ í™•ì‹¤í•˜ì§€ ì•ŠìŒ
```

#### 3. **Exploration ë¶€ì¡±**

```yaml
# V1
initial_log_std: -0.5 # ë‚®ì€ ì´ˆê¸° íƒí—˜
entropy_loss_scale: 0.001 # ë‚®ì€ entropy
rollouts: 64 # ì ì€ rollouts
# ë¬¸ì œ: ê·¸ë¦¬í¼ë¥¼ ë‹«ëŠ” í–‰ë™ì„ ì¶©ë¶„íˆ íƒí—˜í•˜ì§€ ëª»í•¨
```

#### 4. **Networkê°€ ë„ˆë¬´ ë‹¨ìˆœ**

```yaml
# V1
layers: [256, 256, 128]
# ë¬¸ì œ: Pick-and-placeëŠ” ë³µì¡í•œ taskì¸ë° network capacity ë¶€ì¡±
```

---

## âœ¨ V2 ê°œì„  ì‚¬í•­

### 1. **Curriculum Learning ë„ì…**

```python
# V2: 5ë‹¨ê³„ë¡œ ë‚˜ëˆ ì„œ í•™ìŠµ
Stage 1: Reach    (0-100K steps)  â†’ EEë¥¼ ë¬¼ì²´ ê·¼ì²˜ë¡œ
Stage 2: Grasp    (100-200K steps) â†’ ê·¸ë¦¬í¼ ë‹«ê³  ë¬¼ì²´ ì¡ê¸°
Stage 3: Lift     (200-300K steps) â†’ ë¬¼ì²´ ë“¤ì–´ì˜¬ë¦¬ê¸°
Stage 4: Transport(300-400K steps) â†’ ëª©í‘œë¡œ ì´ë™
Stage 5: Place    (400-500K steps) â†’ ëª©í‘œì— ë†“ê¸°

# ê° ë‹¨ê³„ë§ˆë‹¤ ëª…í™•í•œ reward
```

### 2. **ë‹¨ìˆœí•˜ê³  ëª…í™•í•œ Reward**

```python
# V2: í˜„ì¬ stageì— ì§‘ì¤‘í•œ reward
if current_stage == 1:  # Reach
    reward = -2.0 * d_ee_obj + 3.0 * (d_ee_obj < 0.06)

elif current_stage == 2:  # Grasp
    reward = 2.0 * gripper_closure + 5.0 * grasped

elif current_stage == 3:  # Lift
    reward = 4.0 * lift_height + 3.0 * (height > threshold)

# ... ê° stageë§ˆë‹¤ ëª…í™•í•œ ëª©í‘œ
```

### 3. **ë” ê°•í•œ Exploration**

```yaml
# V2
initial_log_std: 0.0 # ë” ë†’ì€ ì´ˆê¸° íƒí—˜
entropy_loss_scale: 0.005 # 5ë°° ë†’ì€ entropy
rollouts: 128 # 2ë°° ë§ì€ rollouts
# íš¨ê³¼: ê·¸ë¦¬í¼ ë‹«ê¸°, ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ë²• íƒí—˜
```

### 4. **ë” ê¹Šì€ Network**

```yaml
# V2
layers: [512, 512, 256, 128]
# íš¨ê³¼: ë³µì¡í•œ manipulation policy í•™ìŠµ ê°€ëŠ¥
```

### 5. **ë¬¼ë¦¬ íŒŒë¼ë¯¸í„° ìµœì í™”**

```python
# V2: Graspí•˜ê¸° ì‰½ê²Œ ì¡°ì •
object:
  mass: 0.03kg    # V1: 0.05kg â†’ ë” ê°€ë²¼ì›€
  size: 0.025m    # V1: 0.03m â†’ ë” ì‘ìŒ

gripper:
  damping: 15.0   # V1: 10.0 â†’ ë” ì•ˆì •ì 
  effort: 30.0    # V1: 20.0 â†’ ë” ê°•í•¨
```

---

## ğŸ“Š V1 vs V2 ì„±ëŠ¥ ì˜ˆìƒ

### V1 (3ì‹œê°„ í•™ìŠµ ì‹¤ì œ ê²°ê³¼)

```
âœ… Reach: 80-90% ì„±ê³µ
âŒ Grasp: 5-10% ì„±ê³µ
âŒ Lift: 0-5% ì„±ê³µ
âŒ Place: 0% ì„±ê³µ

Total Reward: ~10-20 (ë‚®ìŒ)
```

### V2 (ì˜ˆìƒ ê²°ê³¼)

```
50K steps  (~1h):  Reach 90%+
150K steps (~3h):  Grasp 50%+
250K steps (~5h):  Lift 40%+
350K steps (~7h):  Transport 30%+
500K steps (~9h):  Place 20-40%

Total Reward: ~50-100 (ë†’ìŒ)
```

---

## ğŸ¯ êµ¬ì²´ì ì¸ V2 ì‚¬ìš©ë²•

### 1ë‹¨ê³„: V2 í•™ìŠµ ì‹œì‘

```bash
cd C:\Users\Zoe_Lowell\Documents\GitHub\DofBot-Issac-Sim\rl\dofbot_isaacLab\dofbot

python scripts/skrl/train.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --algorithm=PPO \
  --ml_framework=torch \
  --num_envs=1024 \
  --device=cuda
```

### 2ë‹¨ê³„: TensorBoard ëª¨ë‹ˆí„°ë§

```bash
# ìƒˆ cmd ì°½ ì—´ê¸°
cd C:\Users\Zoe_Lowell\Documents\GitHub\DofBot-Issac-Sim\rl\dofbot_isaacLab\dofbot

tensorboard --logdir=logs/skrl/dofbot_pickplace_direct_v2
```

### 3ë‹¨ê³„: í•™ìŠµ ì§„í–‰ í™•ì¸

```
1ì‹œê°„ í›„: Reach rewardê°€ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸
3ì‹œê°„ í›„: Grasp rewardê°€ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
5ì‹œê°„ í›„: Lift rewardê°€ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸
7ì‹œê°„ í›„: Transport rewardê°€ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
9ì‹œê°„ í›„: ìµœì¢… í‰ê°€
```

### 4ë‹¨ê³„: ê²°ê³¼ í‰ê°€

```bash
python scripts/skrl/eval.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --num_envs=64 \
  --checkpoint=logs/skrl/dofbot_pickplace_direct_v2/.../checkpoints/best_agent.pt
```

---

## ğŸ”§ V2ì—ì„œë„ ì•ˆë˜ë©´?

### ë¬¸ì œ 1: ReachëŠ” ë˜ëŠ”ë° Graspì´ ì—¬ì „íˆ ì•ˆë¨

**ì§„ë‹¨:**

```python
# TensorBoardì—ì„œ í™•ì¸
- Stage1 rewardëŠ” ì¦ê°€
- Stage2 rewardëŠ” ê±°ì˜ 0
```

**í•´ê²°ì±… A: Grasp reward ëŒ€í­ ì¦ê°€**

```python
# dofbot_pickplace_env_cfg_v2.py ìˆ˜ì •
rew_stage2_grasp_bonus = 15.0  # 5.0 â†’ 15.0 (3ë°°)
rew_stage2_close_gripper = 5.0  # 2.0 â†’ 5.0
```

**í•´ê²°ì±… B: Objectë¥¼ ë” ì‰½ê²Œ**

```python
# Object ë” ê°€ë³ê³  í¬ê²Œ
object_cfg = RigidObjectCfg(
    spawn=sim_utils.CuboidCfg(
        size=(0.03, 0.03, 0.03),  # 0.025 â†’ 0.03
        mass_props=sim_utils.MassPropertiesCfg(mass=0.02),  # 0.03 â†’ 0.02
    )
)
```

**í•´ê²°ì±… C: ê·¸ë¦¬í¼ë¥¼ ë” ê°•í•˜ê²Œ**

```python
# Gripper actuator ê°•í™”
"gripper": ImplicitActuatorCfg(
    damping=20.0,  # 15.0 â†’ 20.0
    effort_limit_sim=40.0,  # 30.0 â†’ 40.0
)
```

### ë¬¸ì œ 2: Graspì€ ë˜ëŠ”ë° Liftê°€ ì•ˆë¨

**í•´ê²°ì±…:**

```python
# Lift reward ì¦ê°€
rew_stage3_lift = 8.0  # 4.0 â†’ 8.0
rew_stage3_bonus = 5.0  # 3.0 â†’ 5.0

# Arm actuator ê°•í™”
"arm": ImplicitActuatorCfg(
    damping=60.0,  # 50.0 â†’ 60.0
    effort_limit_sim=60.0,  # 50.0 â†’ 60.0
)
```

### ë¬¸ì œ 3: í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

**í•´ê²°ì±…:**

```yaml
# agents/skrl_ppo_pickplace_v2_cfg.yaml

# Learning rate ì¦ê°€
learning_rate: 3.0e-04  # 1e-4 â†’ 3e-4

# Mini-batches ì¦ê°€
mini_batches: 32  # 16 â†’ 32

# í™˜ê²½ ìˆ˜ ì¦ê°€
--num_envs=2048  # 1024 â†’ 2048
```

---

## ğŸ“ ì™œ V2ê°€ ë” ë‚˜ì„ê¹Œ? (ì´ë¡ ì  ë°°ê²½)

### 1. Curriculum Learning

```
ë…¼ë¬¸: "Automatic Goal Generation for Reinforcement Learning Agents"

í•µì‹¬: ë³µì¡í•œ taskë¥¼ ì‘ì€ subtaskë¡œ ë‚˜ëˆ„ë©´ í•™ìŠµì´ í›¨ì”¬ ë¹ ë¦„

Pick-and-placeëŠ” ë³¸ì§ˆì ìœ¼ë¡œ:
Reach â†’ Grasp â†’ Lift â†’ Transport â†’ Place
ì˜ ìˆœì„œê°€ ìˆëŠ” task

V1: ëª¨ë“  ê²ƒì„ í•œë²ˆì— í•™ìŠµ â†’ ì‹¤íŒ¨
V2: ë‹¨ê³„ë³„ë¡œ í•™ìŠµ â†’ ì„±ê³µ ê°€ëŠ¥ì„± ë†’ìŒ
```

### 2. Reward Shaping

```
ë…¼ë¬¸: "Policy Invariance Under Reward Transformations"

í•µì‹¬: Sparse rewardëŠ” í•™ìŠµì´ ì–´ë µê³ ,
      Dense rewardëŠ” local optimaì— ë¹ ì§€ê¸° ì‰¬ì›€

V2 ì ‘ê·¼:
- Sparse bonus (í° ë³´ìƒ, ê°€ë”)
- Dense shaping (ì‘ì€ ë³´ìƒ, ìì£¼)
- ë‘ ê°€ì§€ë¥¼ stageë³„ë¡œ ì¡°í•©
```

### 3. Exploration

```
ë…¼ë¬¸: "Exploration by Random Network Distillation"

í•µì‹¬: Manipulation taskëŠ” ì¶©ë¶„í•œ exploration í•„ìš”

V2 ê°œì„ :
- Higher entropy â†’ ë” ë‹¤ì–‘í•œ í–‰ë™ ì‹œë„
- More rollouts â†’ ë” ë§ì€ ê²½í—˜ ìˆ˜ì§‘
- Longer episodes â†’ ì¶©ë¶„í•œ ì‹œë„ ì‹œê°„
```

---

## ğŸ“ˆ ì‹¤ì „ íŒ

### Tip 1: ì¤‘ê°„ í‰ê°€ë¡œ í•™ìŠµ ë°©í–¥ í™•ì¸

```bash
# 100K stepsë§ˆë‹¤ í‰ê°€
python scripts/skrl/eval.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --num_envs=16 \
  --checkpoint=logs/.../checkpoints/agent_100000.pt

# í™•ì¸ ì‚¬í•­:
# - Reachê°€ ì˜ ë˜ëŠ”ê°€?
# - Grasp ì‹œë„ëŠ” í•˜ëŠ”ê°€?
# - ê·¸ë¦¬í¼ê°€ ë‹«íˆëŠ”ê°€?
```

### Tip 2: TensorBoardë¡œ bottleneck ì°¾ê¸°

```
Reward/Totalì´ ë©ˆì¶˜ ì§€ì  í™•ì¸:
- ~20ì—ì„œ ë©ˆì¶¤: Reach ë‹¨ê³„ ë¬¸ì œ
- ~40ì—ì„œ ë©ˆì¶¤: Grasp ë‹¨ê³„ ë¬¸ì œ
- ~60ì—ì„œ ë©ˆì¶¤: Lift ë‹¨ê³„ ë¬¸ì œ
- ~80ì—ì„œ ë©ˆì¶¤: Transport ë‹¨ê³„ ë¬¸ì œ
```

### Tip 3: í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ë‹¨ê³„ì ìœ¼ë¡œ ì¡°ì •

```
1. ë¨¼ì € default V2ë¡œ í•™ìŠµ
2. TensorBoardë¡œ bottleneck í™•ì¸
3. í•´ë‹¹ stageì˜ rewardë§Œ ì¦ê°€
4. ì¬í•™ìŠµ í›„ ë¹„êµ
5. ë°˜ë³µ
```

---

## ğŸš€ ì§€ê¸ˆ ë‹¹ì¥ ì‹œì‘í•˜ê¸°

```bash
# 1. V2 í•™ìŠµ ì‹œì‘ (ì¶”ì²œ: headless mode)
python scripts/skrl/train.py \
  --task=Dofbot-PickPlace-Direct-v2 \
  --algorithm=PPO \
  --ml_framework=torch \
  --num_envs=1024 \
  --device=cuda \
  --headless

# 2. ë³„ë„ í„°ë¯¸ë„ì—ì„œ TensorBoard
tensorboard --logdir=logs/skrl/dofbot_pickplace_direct_v2

# 3. ë¸Œë¼ìš°ì €ì—ì„œ ëª¨ë‹ˆí„°ë§
# http://localhost:6006

# 4. 7-9ì‹œê°„ í›„ ê²°ê³¼ í™•ì¸!
```

---

## ğŸ“š ì¶”ê°€ ìë£Œ

### V2 ìƒì„¸ ê°€ì´ë“œ

- `V2_TRAINING_GUIDE.md`: V2 í•™ìŠµ ì™„ì „ ê°€ì´ë“œ
- `dofbot_pickplace_env_v2.py`: V2 êµ¬í˜„ ì½”ë“œ
- `dofbot_pickplace_env_cfg_v2.py`: V2 ì„¤ì •

### V1 ì°¸ê³  (ë¹„êµìš©)

- `dofbot_pickplace_env.py`: V1 êµ¬í˜„
- `dofbot_pickplace_env_cfg.py`: V1 ì„¤ì •

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

V2 í•™ìŠµ ì‹œì‘ ì „ í™•ì¸:

- [ ] V1 í•™ìŠµ ì™„ë£Œ (3ì‹œê°„ ëŒë ¤ë³¸ ê²°ê³¼ í™•ì¸)
- [ ] V1ì—ì„œ ReachëŠ” ë˜ì§€ë§Œ Pickì´ ì•ˆë¨ì„ í™•ì¸
- [ ] GPU ì‚¬ìš© ê°€ëŠ¥ (RTX 5070)
- [ ] 7-9ì‹œê°„ í•™ìŠµ ì‹œê°„ í™•ë³´
- [ ] TensorBoard ì‚¬ìš©ë²• ìˆ™ì§€
- [ ] V2_TRAINING_GUIDE.md ì½ìŒ
- [ ] í•™ìŠµ ì¤‘ ëª¨ë‹ˆí„°ë§ ê³„íš ìˆ˜ë¦½

ëª¨ë‘ ì²´í¬ë˜ì—ˆë‹¤ë©´ V2 í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”! ğŸ‰
